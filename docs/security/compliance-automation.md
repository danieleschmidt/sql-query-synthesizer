# Advanced Security Automation & Compliance Framework

**Status**: ðŸ”¶ Enhancement Required - Excellent foundation, needs automation  
**Priority**: P1 - Important for enterprise compliance  
**Current Security Score**: 90/100 (Excellent base, missing automation)  

## Current Security Strengths

### Existing Security Infrastructure âœ…
- **Comprehensive security middleware**: CSRF, rate limiting, security headers
- **Advanced audit logging**: Structured event tracking with configurable retention
- **Input validation**: Multi-layered SQL injection prevention
- **Secrets management**: Environment-based configuration with detect-secrets
- **Container security**: Non-root user, multi-stage builds
- **Development security**: Pre-commit hooks with bandit, safety, detect-secrets

### Security Debt Assessment
- **Technical Debt**: Minimal - well-architected security implementation
- **Process Debt**: Manual security reviews and compliance checks
- **Automation Debt**: Missing automated compliance validation and reporting

## Enhanced Security Automation Framework

### 1. SLSA (Supply Chain Levels for Software Artifacts) Compliance

**File**: `docs/security/slsa-compliance.md`
```markdown
# SLSA Level 3 Compliance Implementation

## Build Integrity Requirements

### Provenance Generation
- **GitHub Actions**: Generate signed build provenance
- **Container Images**: SLSA provenance for Docker images
- **Dependencies**: Verification of dependency provenance

### Build Platform Requirements
- **Isolated Builds**: GitHub-hosted runners (ephemeral)
- **Parameterless Builds**: No build-time parameter injection
- **Hermetic Builds**: Pinned dependencies and base images

### Source Code Requirements
- **Version Control**: Git with signed commits
- **Two-person Review**: Required for main branch
- **Branch Protection**: Prevent force pushes and require status checks

## Implementation Checklist

- [ ] Enable signed commits requirement
- [ ] Configure SLSA GitHub Actions workflow
- [ ] Implement container image signing with cosign
- [ ] Add dependency provenance verification
- [ ] Generate SBOMs (Software Bill of Materials)
- [ ] Implement policy-as-code with OPA (Open Policy Agent)
```

### 2. Automated Security Policy Enforcement

**File**: `.github/workflows/security-policy.yml`
```yaml
name: Security Policy Enforcement

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 6 * * *'  # Daily security scans

jobs:
  security-policy:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for comprehensive analysis
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    # SAST (Static Application Security Testing)
    - name: Run Bandit Security Scan
      run: |
        pip install bandit[toml]
        bandit -r sql_synthesizer/ -f json -o bandit-report.json
        bandit -r sql_synthesizer/ -f txt
    
    - name: Run Semgrep SAST
      uses: returntocorp/semgrep-action@v1
      with:
        config: >-
          p/security-audit
          p/secrets
          p/python
          p/flask
          p/sqlalchemy
        generateSarif: "1"
    
    # Dependency Security
    - name: Run Safety Dependency Check
      run: |
        pip install safety
        safety check --json --output safety-report.json
        safety check  # Also output to console
    
    - name: Run pip-audit
      run: |
        pip install pip-audit
        pip-audit --format=json --output=pip-audit.json
        pip-audit --format=text
    
    # Secrets Detection
    - name: Run detect-secrets
      run: |
        pip install detect-secrets
        detect-secrets scan --all-files --baseline .secrets.baseline
        detect-secrets audit .secrets.baseline
    
    # License Compliance
    - name: License Compliance Check
      run: |
        pip install pip-licenses
        pip-licenses --format=json --output-file=licenses.json
        pip-licenses --fail-on='GPL v3'  # Fail on copyleft licenses
    
    # Container Security
    - name: Build Container for Security Scan
      run: docker build -t sql-synthesizer:security-scan .
    
    - name: Run Trivy Container Scan
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: 'sql-synthesizer:security-scan'
        format: 'sarif'
        output: 'trivy-results.sarif'
    
    - name: Run Grype Container Scan
      uses: anchore/scan-action@v3
      with:
        image: 'sql-synthesizer:security-scan'
        format: sarif
        output-file: grype-results.sarif
    
    # Upload Security Results
    - name: Upload SARIF files
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: |
          semgrep.sarif
          trivy-results.sarif
          grype-results.sarif
    
    # Security Report Generation
    - name: Generate Security Report
      run: |
        python scripts/generate_security_report.py \
          --bandit bandit-report.json \
          --safety safety-report.json \
          --pip-audit pip-audit.json \
          --licenses licenses.json \
          --output security-report.json
    
    - name: Upload Security Report
      uses: actions/upload-artifact@v3
      with:
        name: security-report
        path: |
          security-report.json
          bandit-report.json
          safety-report.json
          pip-audit.json
          licenses.json

  compliance-check:
    runs-on: ubuntu-latest
    needs: security-policy
    
    steps:
    - uses: actions/checkout@v4
    
    - name: SOC 2 Compliance Check
      run: |
        echo "Checking SOC 2 compliance requirements..."
        # Audit logging check
        grep -r "SecurityAuditLogger" sql_synthesizer/ || exit 1
        # Data encryption check
        grep -r "encrypt" sql_synthesizer/config.py || exit 1
        # Access control check
        grep -r "authentication" sql_synthesizer/ || exit 1
    
    - name: GDPR Compliance Check
      run: |
        echo "Checking GDPR compliance requirements..."
        # Data retention policy check
        test -f docs/security/data-retention-policy.md || exit 1
        # Privacy policy check
        test -f PRIVACY.md || exit 1
        # Data deletion capability check
        grep -r "delete.*user.*data" sql_synthesizer/ || exit 1
    
    - name: Generate Compliance Report
      run: |
        python scripts/generate_compliance_report.py \
          --output compliance-report.json
```

### 3. Automated Security Report Generation

**File**: `scripts/generate_security_report.py`
```python
#!/usr/bin/env python3
"""
Automated security report generation for compliance and auditing.
Integrates with existing security audit infrastructure.
"""

import json
import argparse
from datetime import datetime
from pathlib import Path
import subprocess
from typing import Dict, List, Any

class SecurityReportGenerator:
    def __init__(self):
        self.report = {
            "timestamp": datetime.utcnow().isoformat(),
            "repository": "sql_synthesizer",
            "security_posture": "advanced",
            "compliance_frameworks": ["SOC2", "GDPR", "SLSA_L3"],
            "findings": {
                "critical": [],
                "high": [],
                "medium": [],
                "low": [],
                "info": []
            },
            "metrics": {},
            "recommendations": []
        }
    
    def process_bandit_results(self, bandit_file: str):
        """Process Bandit SAST results."""
        if not Path(bandit_file).exists():
            return
            
        with open(bandit_file, 'r') as f:
            bandit_data = json.load(f)
        
        for result in bandit_data.get('results', []):
            severity = result.get('issue_severity', 'UNDEFINED').lower()
            finding = {
                "tool": "bandit",
                "type": "sast",
                "severity": severity,
                "file": result.get('filename', ''),
                "line": result.get('line_number', 0),
                "description": result.get('issue_text', ''),
                "test_id": result.get('test_id', ''),
                "confidence": result.get('issue_confidence', '')
            }
            
            if severity in self.report["findings"]:
                self.report["findings"][severity].append(finding)
        
        self.report["metrics"]["bandit_issues"] = len(bandit_data.get('results', []))
    
    def process_safety_results(self, safety_file: str):
        """Process Safety dependency scan results."""
        if not Path(safety_file).exists():
            return
            
        with open(safety_file, 'r') as f:
            safety_data = json.load(f)
        
        for vuln in safety_data:
            severity = self._determine_cvss_severity(vuln.get('vulnerability', ''))
            finding = {
                "tool": "safety",
                "type": "dependency",
                "severity": severity,
                "package": vuln.get('package', ''),
                "installed_version": vuln.get('installed_version', ''),
                "vulnerability_id": vuln.get('vulnerability_id', ''),
                "description": vuln.get('vulnerability', ''),
                "advisory": vuln.get('advisory', '')
            }
            
            self.report["findings"][severity].append(finding)
        
        self.report["metrics"]["dependency_vulnerabilities"] = len(safety_data)
    
    def process_pip_audit_results(self, pip_audit_file: str):
        """Process pip-audit results."""
        if not Path(pip_audit_file).exists():
            return
            
        with open(pip_audit_file, 'r') as f:
            audit_data = json.load(f)
        
        vulnerabilities = audit_data.get('vulnerabilities', [])
        self.report["metrics"]["pip_audit_vulnerabilities"] = len(vulnerabilities)
        
        for vuln in vulnerabilities:
            severity = self._determine_pip_audit_severity(vuln)
            finding = {
                "tool": "pip-audit",
                "type": "dependency",
                "severity": severity,
                "package": vuln.get('package', ''),
                "installed_version": vuln.get('installed_version', ''),
                "fix_versions": vuln.get('fix_versions', []),
                "description": vuln.get('description', ''),
                "aliases": vuln.get('aliases', [])
            }
            
            self.report["findings"][severity].append(finding)
    
    def process_license_compliance(self, licenses_file: str):
        """Process license compliance results."""
        if not Path(licenses_file).exists():
            return
            
        with open(licenses_file, 'r') as f:
            licenses_data = json.load(f)
        
        # Check for problematic licenses
        problematic_licenses = ['GPL v3', 'AGPL', 'GPL v2']
        license_issues = []
        
        for package in licenses_data:
            license_name = package.get('License', '')
            if any(prob in license_name for prob in problematic_licenses):
                license_issues.append({
                    "package": package.get('Name', ''),
                    "version": package.get('Version', ''),
                    "license": license_name,
                    "issue": "Copyleft license may require code disclosure"
                })
        
        if license_issues:
            for issue in license_issues:
                finding = {
                    "tool": "pip-licenses",
                    "type": "license",
                    "severity": "medium",
                    "package": issue["package"],
                    "description": f"Package uses {issue['license']}: {issue['issue']}"
                }
                self.report["findings"]["medium"].append(finding)
        
        self.report["metrics"]["license_issues"] = len(license_issues)
        self.report["metrics"]["total_dependencies"] = len(licenses_data)
    
    def calculate_security_score(self):
        """Calculate overall security posture score."""
        findings = self.report["findings"]
        
        # Weighted scoring
        score = 100
        score -= len(findings["critical"]) * 20
        score -= len(findings["high"]) * 10
        score -= len(findings["medium"]) * 5
        score -= len(findings["low"]) * 2
        
        self.report["metrics"]["security_score"] = max(0, score)
        return score
    
    def generate_recommendations(self):
        """Generate security recommendations based on findings."""
        recommendations = []
        findings = self.report["findings"]
        
        if findings["critical"]:
            recommendations.append({
                "priority": "immediate",
                "category": "critical_vulnerabilities",
                "description": f"Address {len(findings['critical'])} critical security issues immediately",
                "actions": [
                    "Review and fix critical vulnerabilities",
                    "Consider hotfix deployment if in production",
                    "Update security incident response procedures"
                ]
            })
        
        if findings["high"]:
            recommendations.append({
                "priority": "high",
                "category": "high_vulnerabilities", 
                "description": f"Address {len(findings['high'])} high-severity security issues",
                "actions": [
                    "Schedule fix deployment within 24-48 hours",
                    "Review security testing procedures",
                    "Consider additional security controls"
                ]
            })
        
        if self.report["metrics"].get("dependency_vulnerabilities", 0) > 0:
            recommendations.append({
                "priority": "medium",
                "category": "dependency_management",
                "description": "Implement automated dependency updates",
                "actions": [
                    "Configure Dependabot for automated updates",
                    "Implement automated vulnerability scanning in CI/CD",
                    "Set up dependency security monitoring"
                ]
            })
        
        self.report["recommendations"] = recommendations
    
    def _determine_cvss_severity(self, vulnerability_text: str) -> str:
        """Determine severity based on CVSS score or vulnerability text."""
        # Simple heuristic - in production, parse actual CVSS scores
        if any(keyword in vulnerability_text.lower() for keyword in ['critical', 'remote code execution', 'rce']):
            return 'critical'
        elif any(keyword in vulnerability_text.lower() for keyword in ['high', 'sql injection', 'xss']):
            return 'high'
        elif any(keyword in vulnerability_text.lower() for keyword in ['medium', 'denial of service']):
            return 'medium'
        else:
            return 'low'
    
    def _determine_pip_audit_severity(self, vuln: Dict[str, Any]) -> str:
        """Determine severity for pip-audit vulnerabilities."""
        # Implement CVSS score parsing or alias-based severity determination
        aliases = vuln.get('aliases', [])
        if any('CRITICAL' in alias for alias in aliases):
            return 'critical'
        elif any('HIGH' in alias for alias in aliases):
            return 'high'
        else:
            return 'medium'
    
    def generate_report(self, output_file: str):
        """Generate final security report."""
        self.calculate_security_score()
        self.generate_recommendations()
        
        # Add summary
        self.report["summary"] = {
            "total_findings": sum(len(findings) for findings in self.report["findings"].values()),
            "security_score": self.report["metrics"]["security_score"],
            "compliance_status": "compliant" if self.report["metrics"]["security_score"] >= 80 else "non_compliant",
            "next_assessment_due": (datetime.utcnow().replace(day=1) + 
                                  datetime.timedelta(days=32)).replace(day=1).isoformat()
        }
        
        with open(output_file, 'w') as f:
            json.dump(self.report, f, indent=2)
        
        print(f"Security report generated: {output_file}")
        print(f"Security Score: {self.report['metrics']['security_score']}/100")
        print(f"Total Findings: {self.report['summary']['total_findings']}")

def main():
    parser = argparse.ArgumentParser(description='Generate comprehensive security report')
    parser.add_argument('--bandit', help='Bandit JSON report file')
    parser.add_argument('--safety', help='Safety JSON report file') 
    parser.add_argument('--pip-audit', help='pip-audit JSON report file')
    parser.add_argument('--licenses', help='pip-licenses JSON report file')
    parser.add_argument('--output', required=True, help='Output JSON file')
    
    args = parser.parse_args()
    
    generator = SecurityReportGenerator()
    
    if args.bandit:
        generator.process_bandit_results(args.bandit)
    if args.safety:
        generator.process_safety_results(args.safety)
    if args.pip_audit:
        generator.process_pip_audit_results(args.pip_audit)
    if args.licenses:
        generator.process_license_compliance(args.licenses)
    
    generator.generate_report(args.output)

if __name__ == "__main__":
    main()
```

### 4. Compliance Documentation Automation

**File**: `docs/compliance/soc2-controls.md`
```markdown
# SOC 2 Type II Controls Implementation

## Control Environment (CC1)

### CC1.1: Commitment to Integrity and Ethical Values
- **Implementation**: Code of conduct in CODE_OF_CONDUCT.md
- **Evidence**: Contributor guidelines and review processes
- **Automation**: Pre-commit hooks enforce coding standards

### CC1.2: Board Independence and Oversight
- **Implementation**: CODEOWNERS file defines review authority
- **Evidence**: Required approvals for main branch changes
- **Automation**: GitHub branch protection rules

## Risk Assessment (CC2)

### CC2.1: Risk Identification
- **Implementation**: Automated security scanning in CI/CD
- **Evidence**: Daily security reports and vulnerability assessments
- **Automation**: Bandit, Safety, Semgrep scans

### CC2.2: Risk Analysis
- **Implementation**: WSJF scoring for security vulnerabilities
- **Evidence**: .terragon/config.yaml security boost multipliers
- **Automation**: Automated vulnerability prioritization

## Control Activities (CC3-CC8)

### CC3.1: Policies and Procedures
- **Implementation**: Documented in docs/security/
- **Evidence**: Security hardening guide and procedures
- **Automation**: Policy-as-code with OPA validation

### CC6.1: Logical and Physical Access Controls
- **Implementation**: Multi-layer authentication and authorization
- **Evidence**: SecurityAuditLogger in sql_synthesizer/security_audit.py
- **Automation**: Access control verification in health checks

### CC6.2: Data Security
- **Implementation**: Encryption at rest and in transit
- **Evidence**: Security middleware and database encryption
- **Automation**: Automated security configuration validation

### CC7.1: System Operations
- **Implementation**: Comprehensive monitoring and alerting
- **Evidence**: Prometheus/Grafana monitoring stack
- **Automation**: Health checks and automated recovery procedures

### CC8.1: Change Management
- **Implementation**: GitOps deployment with approval workflows
- **Evidence**: GitHub Actions with manual approvals for production
- **Automation**: Automated testing and rollback procedures
```

### 5. GDPR Compliance Framework

**File**: `docs/compliance/gdpr-compliance.md`
```markdown
# GDPR Compliance Implementation

## Data Processing Principles (Article 5)

### Lawfulness, Fairness, Transparency
- **Implementation**: Privacy policy and consent mechanisms
- **Data Collection**: Explicit consent for SQL query logging
- **Processing Purpose**: Clearly defined in privacy documentation

### Purpose Limitation
- **Implementation**: Data used only for SQL synthesis improvement
- **Evidence**: Documented data processing purposes
- **Controls**: Automated data usage validation

### Data Minimization
- **Implementation**: Collect only necessary query data
- **Evidence**: Configured data retention in security_audit.py
- **Controls**: Automated PII detection and removal

### Accuracy
- **Implementation**: Data validation and correction procedures
- **Evidence**: Input validation in security middleware
- **Controls**: Data quality monitoring

### Storage Limitation
- **Implementation**: Configurable data retention periods
- **Evidence**: AUDIT_LOG_RETENTION_DAYS in config.py
- **Controls**: Automated data deletion procedures

### Security (Article 32)
- **Implementation**: Comprehensive security framework
- **Evidence**: Security audit logging and encryption
- **Controls**: Continuous security monitoring

## Data Subject Rights (Articles 12-23)

### Right of Access (Article 15)
- **Implementation**: API endpoint for data access requests
- **Process**: Automated data export functionality
- **Timeline**: Response within 30 days

### Right to Rectification (Article 16)
- **Implementation**: Data correction API endpoints
- **Process**: Audit trail for all data modifications
- **Validation**: Automated data integrity checks

### Right to Erasure (Article 17)
- **Implementation**: Secure data deletion procedures
- **Process**: Complete data removal with verification
- **Audit**: Deletion confirmation and logging

### Right to Data Portability (Article 20)
- **Implementation**: Standardized data export formats
- **Process**: JSON/CSV export functionality
- **Automation**: Self-service data export portal

## Breach Notification (Articles 33-34)

### Authority Notification (Article 33)
- **Implementation**: Automated breach detection and notification
- **Timeline**: 72-hour notification requirement
- **Process**: Integrated with security monitoring system

### Data Subject Notification (Article 34)
- **Implementation**: Automated notification system for high-risk breaches
- **Criteria**: Risk assessment automation
- **Communication**: Multi-channel notification system
```

### 6. Automated Compliance Monitoring

**File**: `scripts/compliance_monitor.py`
```python
#!/usr/bin/env python3
"""
Automated compliance monitoring for SOC 2 and GDPR requirements.
Integrates with existing security audit infrastructure.
"""

import json
import os
from datetime import datetime, timedelta
from pathlib import Path
import sqlite3
from typing import Dict, List, Any
import subprocess

class ComplianceMonitor:
    def __init__(self, config_path: str = ".terragon/config.yaml"):
        self.config_path = config_path
        self.compliance_report = {
            "timestamp": datetime.utcnow().isoformat(),
            "frameworks": {
                "soc2": {"status": "unknown", "controls": {}, "findings": []},
                "gdpr": {"status": "unknown", "controls": {}, "findings": []},
                "slsa": {"status": "unknown", "level": 0, "findings": []}
            },
            "overall_score": 0,
            "recommendations": []
        }
    
    def check_soc2_compliance(self):
        """Check SOC 2 Type II compliance controls."""
        soc2_findings = []
        
        # CC1.1: Code of conduct exists
        if not Path("CODE_OF_CONDUCT.md").exists():
            soc2_findings.append({
                "control": "CC1.1",
                "finding": "Missing code of conduct",
                "severity": "medium",
                "remediation": "Create CODE_OF_CONDUCT.md"
            })
        
        # CC2.1: Security scanning automation
        if not Path(".github/workflows/security-policy.yml").exists():
            soc2_findings.append({
                "control": "CC2.1", 
                "finding": "Missing automated security scanning",
                "severity": "high",
                "remediation": "Implement GitHub Actions security workflow"
            })
        
        # CC6.1: Access control implementation
        if not self._check_access_controls():
            soc2_findings.append({
                "control": "CC6.1",
                "finding": "Insufficient access control verification",
                "severity": "high", 
                "remediation": "Enhance access control monitoring"
            })
        
        # CC7.1: Monitoring and alerting
        if not self._check_monitoring_setup():
            soc2_findings.append({
                "control": "CC7.1",
                "finding": "Incomplete monitoring and alerting setup",
                "severity": "medium",
                "remediation": "Complete Prometheus/Grafana integration"
            })
        
        # Calculate SOC 2 compliance score
        total_controls = 20  # Total SOC 2 controls
        failed_controls = len(soc2_findings)
        soc2_score = max(0, ((total_controls - failed_controls) / total_controls) * 100)
        
        self.compliance_report["frameworks"]["soc2"] = {
            "status": "compliant" if soc2_score >= 80 else "non_compliant",
            "score": soc2_score,
            "findings": soc2_findings,
            "controls_passed": total_controls - failed_controls,
            "total_controls": total_controls
        }
    
    def check_gdpr_compliance(self):
        """Check GDPR compliance requirements."""
        gdpr_findings = []
        
        # Article 30: Records of processing activities
        if not Path("docs/compliance/data-processing-records.md").exists():
            gdpr_findings.append({
                "article": "30",
                "finding": "Missing records of processing activities",
                "severity": "high",
                "remediation": "Document all data processing activities"
            })
        
        # Article 32: Security of processing
        if not self._check_data_security():
            gdpr_findings.append({
                "article": "32",
                "finding": "Insufficient data security measures",
                "severity": "critical",
                "remediation": "Implement comprehensive data encryption"
            })
        
        # Article 33: Breach notification procedures
        if not Path("docs/security/incident-response.md").exists():
            gdpr_findings.append({
                "article": "33",
                "finding": "Missing breach notification procedures",
                "severity": "high",
                "remediation": "Create incident response documentation"
            })
        
        # Article 17: Right to erasure implementation
        if not self._check_data_deletion_capability():
            gdpr_findings.append({
                "article": "17",
                "finding": "Missing data deletion capabilities",
                "severity": "medium",
                "remediation": "Implement automated data deletion"
            })
        
        # Calculate GDPR compliance score
        total_requirements = 15  # Key GDPR requirements
        failed_requirements = len(gdpr_findings)
        gdpr_score = max(0, ((total_requirements - failed_requirements) / total_requirements) * 100)
        
        self.compliance_report["frameworks"]["gdpr"] = {
            "status": "compliant" if gdpr_score >= 85 else "non_compliant",
            "score": gdpr_score,
            "findings": gdpr_findings,
            "requirements_met": total_requirements - failed_requirements,
            "total_requirements": total_requirements
        }
    
    def check_slsa_compliance(self):
        """Check SLSA (Supply Chain Levels for Software Artifacts) compliance."""
        slsa_findings = []
        current_level = 0
        
        # SLSA Level 1: Build process requirements
        if Path("Dockerfile").exists() and Path(".github/workflows/ci.yml").exists():
            current_level = 1
        else:
            slsa_findings.append({
                "level": "1",
                "finding": "Missing build automation",
                "remediation": "Implement automated build process"
            })
        
        # SLSA Level 2: Build service requirements  
        if current_level >= 1:
            if self._check_build_provenance():
                current_level = 2
            else:
                slsa_findings.append({
                    "level": "2", 
                    "finding": "Missing build provenance generation",
                    "remediation": "Implement build provenance in CI/CD"
                })
        
        # SLSA Level 3: Build platform requirements
        if current_level >= 2:
            if self._check_build_isolation():
                current_level = 3
            else:
                slsa_findings.append({
                    "level": "3",
                    "finding": "Build process not sufficiently isolated",
                    "remediation": "Use GitHub-hosted runners with ephemeral environments"
                })
        
        self.compliance_report["frameworks"]["slsa"] = {
            "status": "compliant" if current_level >= 2 else "non_compliant",
            "level": current_level,
            "target_level": 3,
            "findings": slsa_findings
        }
    
    def _check_access_controls(self) -> bool:
        """Check if access controls are properly implemented."""
        # Check for security audit logging
        security_audit_exists = Path("sql_synthesizer/security_audit.py").exists()
        
        # Check for authentication middleware
        webapp_file = Path("sql_synthesizer/webapp.py")
        if webapp_file.exists():
            content = webapp_file.read_text()
            has_auth = "authentication" in content.lower() or "login" in content.lower()
            return security_audit_exists and has_auth
        
        return False
    
    def _check_monitoring_setup(self) -> bool:
        """Check if monitoring and alerting is properly configured."""
        prometheus_config = Path("monitoring/prometheus/prometheus.yml").exists()
        grafana_config = Path("monitoring/grafana").exists()
        docker_compose = Path("docker-compose.yml").exists()
        
        return prometheus_config and grafana_config and docker_compose
    
    def _check_data_security(self) -> bool:
        """Check data security implementation."""
        # Check for security middleware
        security_file = Path("sql_synthesizer/security.py")
        if security_file.exists():
            content = security_file.read_text()
            has_encryption = "encrypt" in content.lower()
            has_validation = "validate" in content.lower()
            return has_encryption and has_validation
        return False
    
    def _check_data_deletion_capability(self) -> bool:
        """Check if data deletion capabilities exist."""
        # Check for data deletion in database module
        db_file = Path("sql_synthesizer/database.py")
        if db_file.exists():
            content = db_file.read_text()
            return "delete" in content.lower() and "user" in content.lower()
        return False
    
    def _check_build_provenance(self) -> bool:
        """Check if build provenance is generated."""
        # In a real implementation, check for SLSA provenance generation
        github_workflows = Path(".github/workflows")
        if github_workflows.exists():
            for workflow in github_workflows.glob("*.yml"):
                content = workflow.read_text()
                if "provenance" in content.lower():
                    return True
        return False
    
    def _check_build_isolation(self) -> bool:
        """Check if builds are properly isolated."""
        # Check for GitHub Actions usage (provides isolation)
        github_workflows = Path(".github/workflows")
        if github_workflows.exists():
            for workflow in github_workflows.glob("*.yml"):
                content = workflow.read_text()
                if "runs-on: ubuntu-latest" in content:
                    return True
        return False
    
    def calculate_overall_score(self):
        """Calculate overall compliance score."""
        soc2_score = self.compliance_report["frameworks"]["soc2"].get("score", 0)
        gdpr_score = self.compliance_report["frameworks"]["gdpr"].get("score", 0)
        slsa_score = (self.compliance_report["frameworks"]["slsa"].get("level", 0) / 3) * 100
        
        # Weighted average (SOC2: 40%, GDPR: 40%, SLSA: 20%)
        overall_score = (soc2_score * 0.4) + (gdpr_score * 0.4) + (slsa_score * 0.2)
        self.compliance_report["overall_score"] = round(overall_score, 2)
    
    def generate_recommendations(self):
        """Generate compliance improvement recommendations."""
        recommendations = []
        
        # High priority recommendations
        for framework in self.compliance_report["frameworks"].values():
            for finding in framework.get("findings", []):
                if finding.get("severity") in ["critical", "high"]:
                    recommendations.append({
                        "priority": "high",
                        "framework": framework,
                        "finding": finding.get("finding"),
                        "remediation": finding.get("remediation")
                    })
        
        # Sort by priority
        recommendations.sort(key=lambda x: {"critical": 0, "high": 1, "medium": 2, "low": 3}.get(
            x.get("priority", "low"), 3))
        
        self.compliance_report["recommendations"] = recommendations[:10]  # Top 10
    
    def generate_report(self, output_file: str):
        """Generate comprehensive compliance report."""
        self.check_soc2_compliance()
        self.check_gdpr_compliance() 
        self.check_slsa_compliance()
        self.calculate_overall_score()
        self.generate_recommendations()
        
        with open(output_file, 'w') as f:
            json.dump(self.compliance_report, f, indent=2)
        
        print(f"Compliance report generated: {output_file}")
        print(f"Overall Compliance Score: {self.compliance_report['overall_score']}/100")
        
        # Print framework scores
        for name, framework in self.compliance_report["frameworks"].items():
            score = framework.get("score", framework.get("level", 0) * 33.33)
            print(f"{name.upper()} Score: {score:.1f}")

def main():
    import argparse
    parser = argparse.ArgumentParser(description='Monitor compliance status')
    parser.add_argument('--output', default='compliance-report.json', 
                       help='Output file for compliance report')
    
    args = parser.parse_args()
    
    monitor = ComplianceMonitor()
    monitor.generate_report(args.output)

if __name__ == "__main__":
    main()
```

## Implementation Timeline

### Week 1: Security Automation Foundation
- [ ] Implement GitHub Actions security policy workflow
- [ ] Create automated security report generation
- [ ] Set up SLSA provenance generation

### Week 2: Compliance Documentation
- [ ] Complete SOC 2 controls documentation
- [ ] Implement GDPR compliance framework
- [ ] Create compliance monitoring automation

### Week 3: Advanced Security Features
- [ ] Container image signing with cosign
- [ ] Policy-as-code with OPA
- [ ] Advanced threat detection integration

### Week 4: Integration and Testing
- [ ] Integrate all security automation with CI/CD
- [ ] Test compliance monitoring and reporting
- [ ] Document incident response procedures

## Success Metrics

- **Security Score**: Maintain >90/100 automated security score
- **Compliance Score**: Achieve >85/100 overall compliance score
- **Vulnerability Response**: <24 hours for critical, <72 hours for high
- **Audit Readiness**: 100% compliance documentation coverage
- **Incident Response**: <15 minutes detection, <30 minutes initial response